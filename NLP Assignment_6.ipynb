{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00b5da9",
   "metadata": {},
   "source": [
    "1.What are Vanilla autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53908047",
   "metadata": {},
   "source": [
    "Answer- Vanilla autoencoders, also known as basic autoencoders or traditional autoencoders, are a type of neural network architecture used for unsupervised learning and dimensionality reduction. They aim to learn a compressed representation, or encoding, of input data and then reconstruct the original data from this encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7845643",
   "metadata": {},
   "source": [
    "2.What are Sparse autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6b236",
   "metadata": {},
   "source": [
    "Answer- Sparse autoencoders are a type of autoencoder that incorporate sparsity constraints during the training process. The goal of sparse autoencoders is to learn a compressed representation of the input data while also encouraging the activations of the hidden units to be sparse, meaning only a small number of hidden units are active or \"firing\" at any given time.\n",
    "\n",
    "The sparsity constraint in sparse autoencoders is typically imposed on the activations of the hidden units. This encourages the autoencoder to learn a more compact and efficient representation by forcing most of the hidden units to remain inactive or close to zero. Sparse representations are desirable because they tend to capture the most informative and discriminative features of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554ecda",
   "metadata": {},
   "source": [
    "3.What are Denoising autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b259a81",
   "metadata": {},
   "source": [
    "Answer- Denoising autoencoders are a type of autoencoder designed to learn robust representations of input data by reconstructing clean data from corrupted or noisy input. They are trained to remove noise or corruption from the input, thereby enhancing the denoising capability of the model.\n",
    "\n",
    "The basic architecture of a denoising autoencoder is similar to a traditional autoencoder, consisting of an encoder and a decoder. However, during training, the input data is intentionally corrupted by adding noise or applying random transformations. The autoencoder is then trained to reconstruct the original, clean data from the corrupted input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13e6e3",
   "metadata": {},
   "source": [
    "4.What are Convolutional autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c0f81",
   "metadata": {},
   "source": [
    "Answer- Convolutional autoencoders are a variant of autoencoders that leverage convolutional neural network (CNN) architectures for both the encoder and decoder components. They are specifically designed to handle data with a grid-like structure, such as images.\n",
    "\n",
    "The architecture of a convolutional autoencoder consists of convolutional layers for feature extraction and deconvolutional (or transposed convolutional) layers for feature reconstruction. The convolutional layers capture spatial hierarchies and local patterns in the input data, while the deconvolutional layers reverse the convolution operation to reconstruct the original input from the learned features.\n",
    "\n",
    "The key components of a convolutional autoencoder are as follows:\n",
    "\n",
    "1. __Encoder__: The encoder part of a convolutional autoencoder consists of one or more convolutional layers followed by pooling or downsampling layers. These layers progressively reduce the spatial dimensions of the input while increasing the number of feature maps or channels. The encoder learns to extract hierarchical representations of the input data, capturing features at different levels of abstraction.\n",
    "\n",
    "\n",
    "2. __Decoder__: The decoder part of a convolutional autoencoder is responsible for reconstructing the input data from the learned features. It consists of one or more deconvolutional (or transposed convolutional) layers followed by upsampling or unpooling layers. The decoder layers gradually increase the spatial dimensions while reducing the number of feature maps, aiming to reconstruct the original input.\n",
    "\n",
    "\n",
    "3. __Skip Connections__: To improve the reconstruction quality and address the vanishing gradient problem, skip connections can be introduced between corresponding layers of the encoder and decoder. These connections allow the flow of information from early encoder layers directly to the corresponding decoder layers, aiding in the preservation of spatial details and enhancing the overall reconstruction quality.\n",
    "\n",
    "\n",
    "Convolutional autoencoders are particularly effective in capturing local patterns and spatial dependencies in images. They can learn compact and meaningful representations of images, which are useful for tasks such as image denoising, image inpainting, image compression, and image generation. The convolutional architecture enables the autoencoder to exploit the translational equivariance property of CNNs, making it well-suited for handling images and other grid-like data.\n",
    "\n",
    "By leveraging the power of convolutional neural networks, convolutional autoencoders provide an effective framework for unsupervised feature learning and image reconstruction tasks, allowing for efficient representation and manipulation of complex visual data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a6fa9",
   "metadata": {},
   "source": [
    "5.What are Stacked autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b56a0",
   "metadata": {},
   "source": [
    "Answer- Stacked autoencoders, also known as deep autoencoders or deep belief networks, are a type of neural network architecture that consists of multiple layers of autoencoders stacked on top of each other. Each layer of the stacked autoencoder serves as the encoder for the layer above it, while the lower layers serve as decoders for the layers above.\n",
    "\n",
    "The architecture of stacked autoencoders typically includes an input layer, one or more hidden layers, and an output layer. The hidden layers can be seen as a series of encoders and decoders, with each layer learning progressively more abstract representations of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528ffb9",
   "metadata": {},
   "source": [
    "6.Explain how to generate sentences using LSTM autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f7315",
   "metadata": {},
   "source": [
    "Answer- To generate sentences using LSTM autoencoders, you can follow the following steps:\n",
    "\n",
    "1. __Dataset Preparation__: Prepare a dataset of sentences that will serve as the training data for your LSTM autoencoder. Each sentence should be tokenized into individual words or subwords. You will use these sentences to train the autoencoder to reconstruct the input sequences.\n",
    "\n",
    "\n",
    "2. __Preprocessing__: Convert the sentences into numerical representations suitable for training the LSTM autoencoder. This typically involves creating a vocabulary of unique words in the dataset and assigning a unique index to each word. You may also need to pad or truncate the sentences to a fixed length to ensure uniform input size.\n",
    "\n",
    "\n",
    "3. __LSTM Autoencoder Architecture__: Design the architecture of the LSTM autoencoder. It typically consists of an LSTM encoder and an LSTM decoder. The encoder takes the input sentence and encodes it into a fixed-length vector representation, while the decoder reconstructs the sentence from the encoded representation. The encoder and decoder share the same LSTM cell, which allows the model to learn a compressed representation of the input sequence.\n",
    "\n",
    "\n",
    "4. __Training__: Train the LSTM autoencoder using the prepared dataset. The objective is to minimize the reconstruction loss, which measures the difference between the original input sequence and the reconstructed output. During training, feed the input sentence into the encoder, obtain the encoded representation, and then pass it through the decoder to reconstruct the sentence. Update the model's parameters using backpropagation and gradient descent to minimize the reconstruction loss.\n",
    "\n",
    "\n",
    "5. __Generating Sentences__: After training the LSTM autoencoder, you can use it to generate sentences by following these steps:\n",
    "\n",
    "a. Start with a seed sentence or a random initial sequence of words.\n",
    "\n",
    "b. Encode the seed sentence using the trained encoder, which produces a fixed-length vector representation.\n",
    "\n",
    "c. Initialize the decoder with the encoded representation and set it to generate the next word.\n",
    "\n",
    "d. Repeat the following steps until you reach the desired length of the generated sentence or encounter an end-of-sentence token, Pass the current input and the previous hidden state through the decoder. Predict the next word in the sequence using the output of the decoder. Update the input to the decoder with the predicted word. Append the predicted word to the generated sentence.\n",
    "\n",
    "e. Decode the generated sentence from the numerical representation back into words using the vocabulary.\n",
    "\n",
    "By repeating the steps above, you can generate sentences from the LSTM autoencoder. The model learns the patterns and structures of sentences during training, and the generated sentences should exhibit similar patterns and coherence. Adjusting the temperature parameter during the word prediction step can control the randomness and creativity of the generated sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8afc6d",
   "metadata": {},
   "source": [
    "7.Explain Extractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b04d7",
   "metadata": {},
   "source": [
    "Answer- Extractive summarization is a technique in natural language processing that aims to create a summary of a longer document by selecting and combining the most important sentences or phrases from the original text. It involves identifying the most relevant and informative parts of the document and extracting them as a concise summary while preserving the original wording.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c2bcf",
   "metadata": {},
   "source": [
    "8.Explain Abstractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e68142",
   "metadata": {},
   "source": [
    "Answer- Abstractive summarization is a technique in natural language processing that aims to generate a concise summary of a longer document by understanding the meaning and context of the text and generating new, human-like sentences that capture the key information. Unlike extractive summarization, which selects and combines sentences from the original text, abstractive summarization involves paraphrasing and rephrasing the content to create a summary that may not exist in the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978dd9a7",
   "metadata": {},
   "source": [
    "9.Explain Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf655e",
   "metadata": {},
   "source": [
    "Answer- Beam search is a search algorithm commonly used in natural language processing tasks, such as machine translation and text generation, to find the most likely sequence of words given a probabilistic model. It is an extension of the greedy search approach that explores multiple possible sequences rather than just choosing the most probable word at each step.\n",
    "\n",
    "The basic idea behind beam search is to maintain a \"beam\" or a set of the most promising partial sequences during the search process. At each step, the algorithm expands the beam by considering all possible next words for each partial sequence and selects the top-k most probable sequences based on their accumulated probabilities. The parameter k is known as the beam width and determines how many sequences are kept in the beam at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd6629",
   "metadata": {},
   "source": [
    "10.Explain Length normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb9b30",
   "metadata": {},
   "source": [
    "Answer- Length normalization is a technique commonly used in natural language processing, specifically in sequence generation tasks like machine translation or text summarization. It addresses the issue of sentence length bias that can occur when comparing or evaluating sequences of different lengths.\n",
    "\n",
    "When generating sequences, longer sequences tend to have higher probabilities assigned to them compared to shorter sequences. This happens because longer sequences have more opportunities to accumulate probabilities and can be penalized for their length during evaluation or comparison. Length normalization aims to mitigate this bias and provide a fair comparison between sequences of varying lengths.\n",
    "\n",
    "The basic idea behind length normalization is to divide the log-probability of a sequence by its length. This rescales the probabilities based on the length of the sequence, making them comparable across different lengths. By dividing the log-probability by the length, longer sequences are penalized less, and shorter sequences are penalized more, resulting in a more balanced comparison.\n",
    "\n",
    "Mathematically, the length-normalized score for a sequence is calculated as:\n",
    "\n",
    "Normalized_Score = Log_Probability / Length^α\n",
    "\n",
    "where:\n",
    "\n",
    "Log_Probability is the log-probability assigned to the sequence by the model.\n",
    "Length is the length of the sequence.\n",
    "α is a hyperparameter that controls the strength of length normalization. It can be set based on empirical observations or tuned through experimentation.\n",
    "The value of α determines the impact of length normalization. When α is set to 0, no length normalization is applied, and the original log-probabilities are used as is. As α increases, the length normalization effect becomes stronger, and longer sequences are penalized more.\n",
    "\n",
    "Length normalization helps to ensure that the quality of generated sequences is not solely determined by their length. It encourages the generation of concise and coherent sequences by preventing longer sequences from dominating the ranking or evaluation process solely due to their length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e528ed6",
   "metadata": {},
   "source": [
    "11.Explain Coverage normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2a45a",
   "metadata": {},
   "source": [
    "Answer- Coverage normalization is a technique used in sequence generation tasks, such as abstractive summarization or machine translation, to address the issue of repeated or redundant information in the generated sequences. It helps ensure that the generated sequences cover the relevant information from the source text effectively.\n",
    "\n",
    "During sequence generation, a model may generate repeated or duplicated words or phrases, leading to output sequences that lack diversity or fail to capture the complete information present in the source text. Coverage normalization aims to encourage the model to cover as much of the source text as possible, minimizing repetition and promoting the inclusion of unique and relevant content.\n",
    "\n",
    "The coverage vector is a key component in coverage normalization. It is a vector that keeps track of the attention or focus of the model on different parts of the input sequence. At each step of the generation process, the coverage vector is updated to reflect the accumulated attention or coverage of the model on the source text up to that point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6cc30",
   "metadata": {},
   "source": [
    "12.Explain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13b9b5",
   "metadata": {},
   "source": [
    "Answer- ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of evaluation metrics commonly used in natural language processing and text summarization tasks to measure the quality of generated summaries or translations. ROUGE metrics assess the similarity between the generated output and one or more reference summaries or translations.\n",
    "\n",
    "ROUGE is a recall-based metric, meaning it focuses on the coverage of important information in the generated output compared to the reference summaries. The key idea is to measure how well the generated output captures the essential content from the reference summaries.\n",
    "\n",
    "ROUGE consists of several variations, including ROUGE-N, ROUGE-L, ROUGE-S, ROUGE-SU, and others. Here are brief explanations of some of the commonly used ROUGE metrics:\n",
    "\n",
    "1. __ROUGE-N__: ROUGE-N measures the n-gram overlap between the generated output and the reference summaries. It calculates precision, recall, and F1 score based on the number of overlapping n-grams (contiguous sequences of n words) between the generated output and the references.\n",
    "\n",
    "\n",
    "2. __ROUGE-L__: ROUGE-L measures the longest common subsequence (LCS) between the generated output and the reference summaries. It calculates precision, recall, and F1 score based on the length of the LCS, which represents the longest shared sequence of words.\n",
    "\n",
    "\n",
    "3. __ROUGE-S__: ROUGE-S measures the skip-bigram overlap between the generated output and the reference summaries. Skip-bigrams are pairs of words that are not necessarily consecutive in the text but occur within a certain distance of each other. ROUGE-S calculates precision, recall, and F1 score based on the number of overlapping skip-bigrams.\n",
    "\n",
    "\n",
    "4. __ROUGE-SU__: ROUGE-SU combines the skip-bigram and unigram (individual word) measures. It calculates precision, recall, and F1 score based on the combined overlap of skip-bigrams and unigrams between the generated output and the references.\n",
    "\n",
    "To calculate ROUGE scores, the generated output and reference summaries are preprocessed to remove stop words, punctuation, and other non-informative elements. Then, the necessary n-grams or LCS calculations are performed to determine the overlapping units. Precision, recall, and F1 scores are computed based on these overlaps, and the average scores are reported for multiple samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b070058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61036f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6361ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
