{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c24fe30",
   "metadata": {},
   "source": [
    "1.Explain One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488a75f",
   "metadata": {},
   "source": [
    "Answer- One-Hot Encoding is a popular technique used in data preprocessing to convert categorical variables into a binary vector representation. It is often used in machine learning algorithms that cannot directly handle categorical data.\n",
    "\n",
    "The process of One-Hot Encoding involves creating new binary columns for each unique category in the categorical variable. Each column represents a distinct category, and for each observation, only one column will have a value of 1 (indicating the presence of that category), while all other columns will have a value of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc7e6d5",
   "metadata": {},
   "source": [
    "2.Explain Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe70af",
   "metadata": {},
   "source": [
    "Answer- The Bag of Words (BoW) is a popular and simple technique used in natural language processing (NLP) and text mining to represent text documents as numerical feature vectors. It is based on the assumption that the occurrence of words in a document is the key information and the order or structure of the words can be disregarded.\n",
    "\n",
    "The Bag of Words approach involves the following steps:\n",
    "\n",
    "__Tokenization__: The text documents are divided into individual words or tokens. Punctuation marks and special characters are typically removed, and words are converted to lowercase for consistency.\n",
    "\n",
    "__Vocabulary Creation__: A vocabulary is created by collecting all unique words from the corpus of documents. Each word in the vocabulary represents a feature.\n",
    "\n",
    "__Document-Term Matrix__: For each document, a vector representation called the Document-Term Matrix is created. The matrix has one row for each document and one column for each word in the vocabulary. The value in each cell of the matrix represents the frequency of a particular word in the corresponding document.\n",
    "\n",
    "__Feature Encoding__: The frequency values in the Document-Term Matrix can be further processed or encoded to represent the importance or relevance of words. Common encoding techniques include binary encoding (1 if the word is present, 0 otherwise), term frequency (the count of occurrences of a word), and term frequency-inverse document frequency (TF-IDF) that considers the importance of a word in a document relative to the entire corpus.\n",
    "\n",
    "The Bag of Words representation provides a numerical representation of text data that can be used as input to machine learning algorithms. It allows text documents to be analyzed using traditional quantitative techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df27228c",
   "metadata": {},
   "source": [
    "3.Explain Bag of N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54eb49",
   "metadata": {},
   "source": [
    "Answer- The Bag of N-Grams is an extension of the Bag of Words (BoW) approach in natural language processing (NLP) and text mining. While the BoW approach considers individual words as features, the Bag of N-Grams takes into account sequences of words, known as n-grams, as features.\n",
    "\n",
    "An n-gram is a contiguous sequence of n items from a given text. In the context of NLP, these items are typically words, but they can also be characters or other linguistic units. The value of n determines the length of the n-gram. For example, a 2-gram (also known as a bigram) consists of pairs of consecutive words, while a 3-gram (trigram) consists of triples of consecutive words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd695a2",
   "metadata": {},
   "source": [
    "4.Explain TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496c0fa5",
   "metadata": {},
   "source": [
    "Answer- TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic used in natural language processing (NLP) and information retrieval to assess the importance of a term in a document within a larger corpus.\n",
    "\n",
    "TF-IDF takes into account two factors:\n",
    "\n",
    "1. __Term Frequency (TF)__: It measures the frequency of a term within a document. The assumption is that the more frequent a term appears in a document, the more important it is to that document. TF is calculated by dividing the number of occurrences of a term in a document by the total number of terms in the document.\n",
    "\n",
    "\n",
    "2. __Inverse Document Frequency (IDF)__: It measures the rarity or uniqueness of a term in the entire corpus. The intuition is that terms that appear in a small number of documents are more informative and carry more significance. IDF is calculated by taking the logarithm of the ratio of the total number of documents in the corpus to the number of documents containing the term.\n",
    "\n",
    "The TF-IDF score for a term in a document is obtained by multiplying the TF and IDF values. The higher the TF-IDF score, the more important the term is to that particular document in the corpus.\n",
    "\n",
    "The formula for calculating TF-IDF for a term in a document is:\n",
    "\n",
    "TF-IDF = (Term Frequency) * (Inverse Document Frequency)\n",
    "\n",
    "TF-IDF scores can be used in various NLP applications, such as document classification, information retrieval, and text mining. It helps to highlight terms that are both frequent within a document and unique to that document, making them potentially more informative and discriminative.\n",
    "\n",
    "By using TF-IDF, common and less informative words (such as \"the,\" \"and,\" etc.) that appear in many documents tend to have low scores, while specific and significant terms that occur in few documents have higher scores. This allows for better discrimination and relevance ranking when dealing with large collections of documents.\n",
    "\n",
    "Overall, TF-IDF is a widely used technique in NLP for weighting and ranking terms based on their importance in individual documents within a larger corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f28a85",
   "metadata": {},
   "source": [
    "5.What is OOV problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce266804",
   "metadata": {},
   "source": [
    "Answer- Out-Of-Vocabulary (OOV) words is an important problem in NLP, we will introduce how to process words that are out of vocabulary in this tutorial.\n",
    "\n",
    "1. We often use word2vec or glove to process documents to create word vector or word embedding.\n",
    "\n",
    "2. However, we may ignore some words that appear rarely in documents, which may cause OOV problem.\n",
    "\n",
    "3. Meanwhile, we may use some pre-trained word representation file, which may do not contain some words in our data set. It also can cause OOV problem.\n",
    "\n",
    "How to fix OOV problem?\n",
    "There are three main ways that often be used in AI application.\n",
    "\n",
    "-> Ingoring them\n",
    "Generally, words that are out of vocabulary often appear rarely, the will contribute less to our model. The performance of our model will drop scarcely, it means we can ignore them.\n",
    "\n",
    "-> Replacing them using <UNK>\n",
    "We can replace all words that are out of vocabulary by using word <UNK>.\n",
    "\n",
    "->Initializing them by a uniform distribution with range [-0.01, 0.01]\n",
    "Out-Of-Vocabulary (OOV) words can be initialized from a uniform distribution with range [-0.01, 0.01]. We can use this uniform distribution to train our model.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6fc450",
   "metadata": {},
   "source": [
    "6.What are word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f6c1b",
   "metadata": {},
   "source": [
    "Answer- Word embeddings are dense vector representations of words in a high-dimensional vector space. They are mathematical representations that capture the semantic and syntactic meanings of words based on their contextual usage in a given corpus or text dataset. Word embeddings have revolutionized natural language processing (NLP) tasks by enabling machines to understand and process textual data more effectively.\n",
    "\n",
    "Traditional approaches to representing words in NLP, such as one-hot encoding or bag-of-words, have limitations in capturing the semantic relationships and contextual similarities between words. Word embeddings overcome these limitations by learning distributed representations of words where similar words are closer together in the vector space.\n",
    "\n",
    "Word embeddings are typically generated using neural network models, such as Word2Vec, GloVe (Global Vectors for Word Representation), or FastText. These models are trained on large text corpora and learn to predict the context or neighboring words of a target word. During the training process, the model adjusts the word embeddings to optimize this prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084ddef",
   "metadata": {},
   "source": [
    "7.Explain Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ca69b",
   "metadata": {},
   "source": [
    "Answer- Continuous Bag of Words (CBOW) is a model architecture for training word embeddings in natural language processing (NLP). It is a type of neural network-based language model that learns to predict a target word based on its surrounding context words within a given window.\n",
    "\n",
    "The CBOW model aims to capture the distributional semantics of words by leveraging the context in which they appear. It assumes that words occurring in similar contexts tend to have similar meanings. The CBOW model learns to predict a target word by considering the context words as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b48ce1",
   "metadata": {},
   "source": [
    "8.Explain SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733f776",
   "metadata": {},
   "source": [
    "Answer- Skip-gram is a model architecture for training word embeddings in natural language processing (NLP). It is a type of neural network-based language model that aims to learn word representations by predicting the context words given a target word.\n",
    "\n",
    "Unlike the Continuous Bag of Words (CBOW) model that predicts a target word from its surrounding context, the Skip-gram model takes a target word as input and predicts the surrounding context words. The Skip-gram model is designed to capture the local context and capture more fine-grained relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3475e3",
   "metadata": {},
   "source": [
    "9.Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c63669",
   "metadata": {},
   "source": [
    "Answer- GloVe (Global Vectors for Word Representation) is a word embedding model used in natural language processing (NLP) to learn vector representations of words based on their co-occurrence statistics in a corpus. It aims to capture both global and local context information to generate high-quality word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660e48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3a65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8deb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
